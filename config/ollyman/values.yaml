ingress:
  ip: 192.168.1.251

opentelemetry-kube-stack:
  fullnameOverride: ""
  namespaceOverride: ""
  clusterName: ""
  extraEnvs: []
  cleanupJob:
    enabled: true
    image:
      repository: bitnami/kubectl
      tag: latest
      digest: ""
  crds:
    install: true
  clusterRole:
    enabled: true
    annotations: {}
    rules: []

  # {{{ Otel Operator
  opentelemetry-operator:
    enabled: true
    manager:
      collectorImage:
        repository: otel/opentelemetry-collector-k8s
    admissionWebhooks:
      failurePolicy: "Ignore"
    crds:
      create: false
  # }}}

  # {{{ Collector Base Config
  defaultCRConfig:
    enabled: false
    suffix: "collector"
    fullnameOverride: ""
    annotations: {}
    labels: {}
    scrape_configs_file: ""
    managementState: managed
    clusterRoleBinding:
      enabled: true
      clusterRoleName: ""
    # replicas: 1
    mode: deployment
    serviceAccount: ""
    image:
      repository: otel/opentelemetry-collector-k8s
      pullPolicy: IfNotPresent
      tag: ""
      digest: ""
    upgradeStrategy: automatic

    # Configuration options for the collector
    config: {}

    hostNetwork: false
    shareProcessNamespace: false
    priorityClassName: ""
    terminationGracePeriodSeconds: 30

    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "250m"

    nodeSelector: {}
    args: {}
    autoscaler: {}
    #   minReplicas: 1
    #   maxReplicas: 10
    #   targetCPUUtilization: 50

    podDisruptionBudget: {}
    #   maxUnavailable: 1

    securityContext: {}
    #   runAsUser: 1000
    #   capabilities:
    #     drop:
    #       - ALL

    podSecurityContext: {}
    #   runAsUser: 1000

    # Annotations for the collector's pods
    podAnnotations: {}
    #   prometheus.io/scrape: "true"

    targetAllocator: {}
    affinity: {}
    lifecycle: {}
    livenessProbe: {}
    observability: {}
    updateStrategy: {}

    volumeMounts: []
    ports: []
    env: []
    volumeClaimTemplates: []
    tolerations: []
    volumes: []
    initContainers: []
    additionalContainers: []
    topologySpreadConstraints: []
    configmaps: []

    presets:
      logsCollection:
        enabled: false
        includeCollectorLogs: true
        storeCheckpoints: false
        maxRecombineLogSize: 102400
      hostMetrics:
        enabled: false
      kubernetesAttributes:
        enabled: false
        extractAllPodLabels: false
        extractAllPodAnnotations: false
      kubeletMetrics:
        enabled: false
      kubernetesEvents:
        enabled: false
      clusterMetrics:
        enabled: false
  # }}}

  collectors:
    # {{{ Daemonset - logs, kubelet metrics, pod logs, k8s attrs
    daemon: 
      suffix: daemon
      mode: daemonset
      enabled: true
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 250Mi
      # A scrape config file to instruct the daemon collector to pull metrics from any matching targets on the same node with
      # prometheus.io/scrape=true
      # This config also scrapes a running node exporter and the kubelet CAdvisor metrics which aren't currently supported.
      scrape_configs_file: "daemon_scrape_configs.yaml"
      presets:
        logsCollection:
          enabled: true
        kubeletMetrics:
          enabled: true
        hostMetrics:
          enabled: true
        kubernetesAttributes:
          enabled: true
      config:

        receivers:
          otlp:
            protocols:
              grpc:
                endpoint: 0.0.0.0:4317
              http:
                endpoint: 0.0.0.0:4318

        processors:
          batch:
            send_batch_size: 1000
            timeout: 1s
            send_batch_max_size: 1500
          resourcedetection/env:
            detectors: [env]
            timeout: 2s
            override: false
        exporters:
          otlp:
            endpoint: 192.168.1.251:4317
            tls:
              insecure: true

        service:
          pipelines:
            traces:
              receivers:
                - otlp
              processors:
                - resourcedetection/env
                - batch
              exporters:
                - otlp
            metrics:
              receivers:
                - otlp
              processors:
                - resourcedetection/env
                - batch
              exporters:
                - otlp
            logs:
              receivers:
                - otlp
              processors:
                - resourcedetection/env
                - batch
              exporters:
                - otlp
    # }}} 
    # {{{ Cluster Stats
    cluster:
      suffix: cluster-stats
      replicas: 1
      mode: deployment
      enabled: true
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 250Mi
      presets:
        kubernetesAttributes:
          enabled: true
        kubernetesEvents:
          enabled: true
        clusterMetrics:
          enabled: true
      config:
        receivers: {}
        processors:
          batch:
            send_batch_size: 1000
            timeout: 1s
            send_batch_max_size: 1500
          resourcedetection/env:
            detectors: [env]
            timeout: 2s
            override: false
        exporters:
          debug: {}
          otlp:
            endpoint: 192.168.1.251:4317
            tls:
              insecure: true
        service:
          pipelines:
            metrics:
              receivers: [k8s_cluster]
              processors: [resourcedetection/env, batch]
              exporters: [otlp]
            logs:
              receivers: [k8sobjects]
              processors: [resourcedetection/env, batch]
              exporters: [otlp]
  # }}}

  # {{{ Code Instrumentation
  instrumentation:
    enabled: false
    labels: {}
    annotations: {}
    exporter:
      # Upon creation of a tracing collector, edit this endpoint.
      endpoint: http://collector-collector:4317
    resource:
      resourceAttributes: {}
      addK8sUIDAttributes: true

    propagators:
      - tracecontext
      - baggage
      - b3
      - b3multi
      - jaeger
      - xray
      - ottrace
    sampler: {}
    env: []

    java: {}
    nodejs: {}
    python: {}
    dotnet: {}
    go: {}
    apacheHttpd: {}
    nginx: {}
  # }}}
  # {{{ opAMP
  opAMPBridge:
    enabled: false
    addReportingLabel: true
    addManagedLabel: false
    endpoint: http://opamp-server:8080
    headers: {}
    capabilities:
      AcceptsOpAMPConnectionSettings: true
      AcceptsOtherConnectionSettings: true
      AcceptsRemoteConfig: true
      AcceptsRestartCommand: true
      ReportsEffectiveConfig: true
      ReportsHealth: true
      ReportsOwnLogs: true
      ReportsOwnMetrics: true
      ReportsOwnTraces: true
      ReportsRemoteConfig: true
      ReportsStatus: true
    componentsAllowed: {}
    resources:
      limits:
        cpu: "250m"
        memory: "256Mi"
      requests:
        cpu: "250m"
        memory: "256Mi"
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
    podSecurityContext:
      fsGroup: 1000
    podAnnotations: {}
    serviceAccount: ""

    image:
      repository: ghcr.io/open-telemetry/opentelemetry-operator/operator-opamp-bridge
      pullPolicy: IfNotPresent
      tag: ""
      digest: ""

    upgradeStrategy: automatic
    volumeMounts: []

    ports: []
    env: []
    envFrom: []
    tolerations: []
    volumes: []
    hostNetwork: false
    priorityClassName: ""
    affinity: {}

    topologySpreadConstraints: []
    clusterRole:
      enabled: true
      annotations: {}
      rules: []
  # }}}
  kubernetesServiceMonitors:
    enabled: false
    ignoreNamespaceSelectors: false
  ## {{{ k8s api server
  kubeApiServer:
    enabled: false
    tlsConfig:
      serverName: kubernetes
      insecureSkipVerify: false
    serviceMonitor:
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      jobLabel: component
      selector:
        matchLabels:
          component: apiserver
          provider: kubernetes
      metricRelabelings:
        # Drop excessively noisy apiserver buckets.
        - action: drop
          regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
          sourceLabels:
            - __name__
            - le
      relabelings: []
      additionalLabels: {}
  # }}}
  # {{{ Kubelet
  kubelet:
    enabled: false
    namespace: kube-system
    serviceMonitor:
      interval: ""
      honorLabels: true
      honorTimestamps: true
      https: true
      cAdvisor: true
      probes: true
  # }}}
  ## {{{ kubeControllerManager
  kubeControllerManager:
    enabled: false
    endpoints: []
    service:
      enabled: true
      port: null
      targetPort: null

    serviceMonitor:
      enabled: true
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      port: http-metrics
      jobLabel: jobLabel
      selector: {}
      https: null
      insecureSkipVerify: null
      serverName: null
      metricRelabelings: []
      relabelings: []
      additionalLabels: {}
  # }}}
  # {{{ CoreDNS
  coreDns:
    enabled: false
    endpoints: []
    service:
      enabled: true
      port: 9153
      targetPort: 9153
    serviceMonitor:
      enabled: true
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      port: http-metrics
      jobLabel: jobLabel
      selector: {}
      metricRelabelings: []
      relabelings: []
      additionalLabels: {}
  # }}}
  # {{{ KubeDNS
  kubeDns:
    enabled: false
    service:
      dnsmasq:
        port: 10054
        targetPort: 10054
      skydns:
        port: 10055
        targetPort: 10055
    serviceMonitor:
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      jobLabel: jobLabel
      selector: {}
      metricRelabelings: []
      relabelings: []
      dnsmasqMetricRelabelings: []
      dnsmasqRelabelings: []
      additionalLabels: {}
  # }}}
  # {{{ Etcd
  kubeEtcd:
    enabled: false
    ## If your etcd is not deployed as a pod, specify IPs it can be found on
    endpoints: []
    service:
      enabled: true
      port: 2381
      targetPort: 2381
    serviceMonitor:
      enabled: true
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      scheme: http
      insecureSkipVerify: false
      serverName: ""
      caFile: ""
      certFile: ""
      keyFile: ""
      port: http-metrics
      jobLabel: jobLabel
      selector: {}
      metricRelabelings: []
      relabelings: []
      additionalLabels: {}
  # }}}
  # {{{ Kube Scheduler
  kubeScheduler:
    enabled: false
    endpoints: []
    service:
      enabled: true
      port: null
      targetPort: null

    serviceMonitor:
      enabled: true
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      https: null
      port: http-metrics
      jobLabel: jobLabel
      selector: {}
      insecureSkipVerify: null
      serverName: null
      metricRelabelings: []
      relabelings: []
      additionalLabels: {}
  # }}}
  # {{{ KubeProxy - Disabled used with cilium
  kubeProxy:
    enabled: false
    endpoints: []
    service:
      enabled: true
      port: 10249
      targetPort: 10249
    serviceMonitor:
      enabled: true
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      proxyUrl: ""
      port: http-metrics
      jobLabel: jobLabel
      selector: {}
      https: false
      metricRelabelings: []
      relabelings: []
      additionalLabels: {}
  # }}}

  kubeStateMetrics:
    enabled: false
  ## {{{ Kube State Metrics
  ## https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
  kube-state-metrics:
    namespaceOverride: ""
    rbac:
      create: true
    releaseLabel: true
    prometheus:
      monitor:
        enabled: true
        interval: ""
        sampleLimit: 0
        targetLimit: 0
        labelLimit: 0
        labelNameLengthLimit: 0
        labelValueLengthLimit: 0
        scrapeTimeout: ""
        proxyUrl: ""
        honorLabels: true
        metricRelabelings: []
        relabelings: []
    selfMonitor:
      enabled: false
  # }}}

  ##  {{{ Node Exporter
  ## Controls whether the prometheus-node-exporter chart should be created.
  ## This block matches the configuration for the kube-prometheus-stack chart for compatibility.
  nodeExporter:
    enabled: false

  ## Configuration for prometheus-node-exporter subchart
  ## This will install a daemonset that pulls metric data from each node
  ## Read more here about the chart:
  ## https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
  prometheus-node-exporter:
    namespaceOverride: ""
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    releaseLabel: true
    extraArgs:
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    service:
      portName: http-metrics
    prometheus:
      monitor:
        enabled: true

        jobLabel: jobLabel

        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""

        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0

        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0

        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0

        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0

        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0

        ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
        ##
        scrapeTimeout: ""

        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - sourceLabels: [__name__]
        #   separator: ;
        #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
        #   replacement: $1
        #   action: drop

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
    rbac:
      ## If true, create PSPs for node-exporter
      ##
      pspEnabled: false
  # }}}
